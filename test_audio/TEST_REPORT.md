# Step-Audio-R1.1 全面测试报告

> 测试日期: 2026-01-18 10:18-10:30  
> 测试环境: 4× NVIDIA L40S (46GB each, 总计 184GB)  
> 模型版本: Step-Audio-R1.1  
> 当前配置: max_model_len=65536, max_num_seqs=2

---

## 1. 测试概述

本次测试对 Step-Audio-R1.1 进行了全面的 API 功能验证和性能基准测试，包括：
- 不同时长音频处理能力测试 (5/10/30/60/85 分钟)
- 所有处理模式功能验证 (s2t/asr/understand/translate/summarize)
- 音频信息 API 测试
- 长音频支持能力分析

---

## 2. 测试音频准备

| 文件名 | 时长 | 文件大小 | 采样率 | 声道 |
|--------|------|----------|--------|------|
| 5min.wav | 5:00 | 9.2 MB | 16kHz | 单声道 |
| 10min.wav | 10:00 | 18.3 MB | 16kHz | 单声道 |
| 30min.wav | 30:00 | 54.9 MB | 16kHz | 单声道 |
| 60min.wav | 60:00 | 109.9 MB | 16kHz | 单声道 |
| 85min.wav | 84:48 | 155.3 MB | 16kHz | 单声道 |

音频来源: Aeron Deep Dive 技术分享录音

---

## 3. API 功能测试结果

### 3.1 音频信息 API (`/api/audio/info`)

| 音频 | 时长(秒) | 采样率 | 声道 | 状态 |
|------|----------|--------|------|------|
| 5min.wav | 300.0 | 16000 | 1 | ✅ |
| 10min.wav | 600.0 | 16000 | 1 | ✅ |
| 30min.wav | 1800.0 | 16000 | 1 | ✅ |
| 60min.wav | 3600.0 | 16000 | 1 | ✅ |
| 85min.wav | 5087.9 | 16000 | 1 | ✅ |

### 3.2 不同时长 s2t 模式测试

| 音频 | 耗时(秒) | 思考长度 | 回答长度 | 状态 |
|------|----------|----------|----------|------|
| 5min.wav | 66.86 | 1,132 | 1,736 | ✅ 成功 |
| 10min.wav | 53.32 | 3,196 | 2,055 | ✅ 成功 |
| 30min.wav | 76.17 | 3,024 | 2,789 | ✅ 成功 |
| 60min.wav | 70.44 | 814 | 1,727 | ✅ 成功 |
| 85min.wav | 71.14 | 0 | 2,639 | ✅ 成功 |

**关键发现**:
- 所有时长音频均成功处理，包括 85 分钟完整音频
- 处理耗时稳定在 50-80 秒范围内，与音频时长无明显线性关系
- 85 分钟音频思考长度为 0，可能是模型直接输出结果

### 3.3 所有处理模式测试 (5min.wav)

| 模式 | 描述 | 耗时(秒) | 思考长度 | 回答长度 | 状态 |
|------|------|----------|----------|----------|------|
| s2t | 语音转文字+摘要 | 66.86 | 1,132 | 1,736 | ✅ |
| asr | 纯语音识别 | 44.31 | 697 | 1,770 | ✅ |
| understand | 内容理解 | 43.71 | 972 | 1,584 | ✅ |
| translate | 翻译(→英文) | 63.63 | 7,342 | 2,134 | ✅ |
| summarize | 摘要 | 24.06 | 2,335 | 1,387 | ✅ |

**关键发现**:
- 所有 5 种处理模式均正常工作
- summarize 模式最快 (24秒)，translate 模式思考最深入 (7342字符)
- asr 模式输出逐字转录，保留口语特征

---

## 4. 显存使用分析

### 4.1 空闲状态 (模型加载后)
| GPU | 已用 | 总量 | 利用率 |
|-----|------|------|--------|
| GPU 0 | 38,993 MB | 46,068 MB | 84.6% |
| GPU 1 | 37,669 MB | 46,068 MB | 81.8% |
| GPU 2 | 37,669 MB | 46,068 MB | 81.8% |
| GPU 3 | 38,139 MB | 46,068 MB | 82.8% |
| **总计** | **152,470 MB** | **184,272 MB** | **82.7%** |

### 4.2 处理 85 分钟音频时
| GPU | 已用 | 总量 | 利用率 |
|-----|------|------|--------|
| GPU 0 | 42,999 MB | 46,068 MB | 93.3% |
| GPU 1 | 41,675 MB | 46,068 MB | 90.5% |
| GPU 2 | 41,675 MB | 46,068 MB | 90.5% |
| GPU 3 | 42,145 MB | 46,068 MB | 91.5% |
| **总计** | **168,494 MB** | **184,272 MB** | **91.4%** |

**结论**: 处理 85 分钟音频时显存增加约 16GB，仍有约 16GB 余量。

---

## 5. 长音频支持能力分析

### 5.1 理论计算公式

```
音频编码率 = 12.5 Hz (每秒 12.5 个 token)
系统预留 token ≈ 1500

最大音频秒数 = (max_model_len - 1500) / 12.5
```

### 5.2 不同配置下的音频时长上限

| max_model_len | max_num_seqs | 最大音频时长 | 预计显存 | 可行性 |
|---------------|--------------|--------------|----------|--------|
| 65,536 | 2 | **85 分钟** | ~168 GB | ✅ 当前配置 |
| 98,304 | 1 | **129 分钟 (2.15h)** | ~175 GB | ✅ 可行 |
| 131,072 | 1 | **173 分钟 (2.88h)** | ~180 GB | ⚠️ 接近上限 |
| 163,840 | 1 | **216 分钟 (3.6h)** | ~185 GB | ❌ 超出显存 |

### 5.3 推荐配置方案

#### 方案 A: 平衡配置 (当前)
```yaml
--max-model-len 65536
--max-num-seqs 2
--gpu-memory-utilization 0.85
```
- 最大音频: 85 分钟
- 并发: 2 请求
- 适用: 常规使用场景

#### 方案 B: 长音频配置
```yaml
--max-model-len 131072
--max-num-seqs 1
--gpu-memory-utilization 0.92
```
- 最大音频: **173 分钟 (约 3 小时)**
- 并发: 1 请求
- 适用: 需要处理超长音频的场景

#### 方案 C: 超长音频 (分段处理)
对于超过 3 小时的音频，使用 `long_audio_processor.py` 分段处理：
```bash
python long_audio_processor.py --input long.mp3 --mode s2t --segment-duration 1800
```

---

## 6. 关于懒加载和显存管理的分析

### 6.1 当前问题

当前 vLLM 服务启动后会立即加载模型到显存，占用约 152GB：
- 模型权重: ~63GB (分布在 4 卡)
- KV Cache 预分配: ~89GB
- 无任务时显存不会释放

### 6.2 懒加载方案建议

**方案 1: CPU 内存预加载 + 按需加载到 GPU**

```python
# 伪代码示意
class LazyModelLoader:
    def __init__(self):
        self.cpu_model = None  # 常驻内存
        self.gpu_model = None  # 按需加载
    
    def preload_to_cpu(self):
        """启动时预加载到 CPU 内存"""
        self.cpu_model = load_model_to_cpu()  # ~63GB 内存
    
    def load_to_gpu(self):
        """有任务时加载到 GPU"""
        if self.gpu_model is None:
            self.gpu_model = self.cpu_model.to('cuda')
    
    def unload_from_gpu(self):
        """空闲时卸载"""
        if self.gpu_model is not None:
            self.gpu_model = None
            torch.cuda.empty_cache()
```

**优点**: 
- 从内存加载到显存速度快 (~10-20秒)
- 空闲时释放显存给其他任务

**缺点**:
- 需要 ~63GB 系统内存常驻
- vLLM 当前不原生支持此模式

**方案 2: 使用 vLLM 的 `--swap-space` 参数**

```bash
vllm serve /model \
  --swap-space 32 \  # 32GB swap 空间
  --gpu-memory-utilization 0.7
```

**方案 3: 容器级别的懒启动**

```yaml
# docker-compose.yml
services:
  vllm:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    # 使用 autoscaler 或 serverless 框架按需启动
```

### 6.3 实现建议

考虑到 vLLM 的架构限制，推荐的折中方案：

1. **短期**: 保持当前配置，接受显存常驻
2. **中期**: 实现一个 wrapper 服务，在无请求 N 分钟后停止 vLLM 容器，有请求时自动启动
3. **长期**: 等待 vLLM 官方支持动态显存管理

---

## 7. 结论与建议

### 7.1 测试结论

| 项目 | 结果 |
|------|------|
| API 功能完整性 | ✅ 所有 5 种模式正常工作 |
| 85 分钟音频支持 | ✅ 稳定处理，耗时 ~71 秒 |
| 最大音频时长 (当前配置) | 85 分钟 |
| 最大音频时长 (优化配置) | **173 分钟 (约 3 小时)** |
| 处理性能 | 50-80 秒/请求，与音频时长弱相关 |

### 7.2 配置建议

1. **日常使用**: 保持当前配置 (max_model_len=65536, max_num_seqs=2)
2. **长音频场景**: 切换到 `docker-compose.3h.yml` 配置
3. **超长音频 (>3h)**: 使用分段处理器

### 7.3 后续优化方向

1. 实现懒加载/按需卸载机制
2. 测试 max_model_len=131072 配置的稳定性
3. 监控长时间运行的显存泄漏情况

---

*报告生成时间: 2026-01-18 10:30 CST*
