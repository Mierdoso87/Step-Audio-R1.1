services:
  vllm:
    image: stepfun2025/vllm:step-audio-2-v20250909
    container_name: step-audio-r1.1-vllm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
    volumes:
      - ${MODEL_PATH:-./Step-Audio-R1.1}:/model:ro
    ports:
      - "0.0.0.0:${VLLM_PORT:-9101}:9999"
    command: >
      vllm serve /model
      --served-model-name Step-Audio-R1.1
      --port 9999
      --host 0.0.0.0
      --max-model-len 131072
      --max-num-seqs 1
      --tensor-parallel-size 4
      --gpu-memory-utilization 0.92
      --trust-remote-code
      --enable-log-requests
      --interleave-mm-strings
      --chat-template '{%- macro render_content(content) -%}{%- if content is string -%}{{- content.replace("<audio_patch>\n", "<audio_patch>") -}}{%- elif content is mapping -%}{{- content['"'"'value'"'"'] if '"'"'value'"'"' in content else content['"'"'text'"'"'] -}}{%- elif content is iterable -%}{%- for item in content -%}{%- if item.type == '"'"'text'"'"' -%}{{- item['"'"'value'"'"'] if '"'"'value'"'"' in item else item['"'"'text'"'"'] -}}{%- elif item.type == '"'"'audio'"'"' -%}<audio_patch>{%- endif -%}{%- endfor -%}{%- endif -%}{%- endmacro -%}{%- if tools -%}{{- '"'"'<|BOT|>system\n'"'"' -}}{%- if messages[0]['"'"'role'"'"'] == '"'"'system'"'"' -%}{{- render_content(messages[0]['"'"'content'"'"']) + '"'"'<|EOT|>'"'"' -}}{%- endif -%}{{- '"'"'<|BOT|>tool_json_schemas\n'"'"' + tools|tojson + '"'"'<|EOT|>'"'"' -}}{%- else -%}{%- if messages[0]['"'"'role'"'"'] == '"'"'system'"'"' -%}{{- '"'"'<|BOT|>system\n'"'"' + render_content(messages[0]['"'"'content'"'"']) + '"'"'<|EOT|>'"'"' -}}{%- endif -%}{%- endif -%}{%- for message in messages -%}{%- if message["role"] == "user" -%}{{- '"'"'<|BOT|>human\n'"'"' + render_content(message["content"]) + '"'"'<|EOT|>'"'"' -}}{%- elif message["role"] == "assistant" -%}{{- '"'"'<|BOT|>assistant\n'"'"' + (render_content(message["content"]) if message["content"] else '"'"''"'"') -}}{%- set is_last_assistant = true -%}{%- for m in messages[loop.index:] -%}{%- if m["role"] == "assistant" -%}{%- set is_last_assistant = false -%}{%- endif -%}{%- endfor -%}{%- if not is_last_assistant -%}{{- '"'"'<|EOT|>'"'"' -}}{%- endif -%}{%- elif message["role"] == "function_output" -%}{%- else -%}{%- if not (loop.first and message["role"] == "system") -%}{{- '"'"'<|BOT|>'"'"' + message["role"] + '"'"'\n'"'"' + render_content(message["content"]) + '"'"'<|EOT|>'"'"' -}}{%- endif -%}{%- endif -%}{%- endfor -%}{%- if add_generation_prompt -%}{{- '"'"'<|BOT|>assistant\n<think>\n'"'"' -}}{%- endif -%}'
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9999/health || exit 1"]
      interval: 60s
      timeout: 30s
      retries: 10
      start_period: 300s

  web:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: step-audio-r1.1-web
    environment:
      - VLLM_API_URL=http://vllm:9999/v1/chat/completions
      - MODEL_NAME=Step-Audio-R1.1
      - PORT=9100
    ports:
      - "0.0.0.0:${WEB_PORT:-9100}:9100"
    volumes:
      - /tmp/step-audio-r1:/tmp/step-audio-r1
    depends_on:
      vllm:
        condition: service_healthy
    restart: unless-stopped
