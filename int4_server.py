#!/usr/bin/env python3
"""
Step-Audio-R1.1 INT4 é‡åŒ–æ¨ç†æœåŠ¡
ä½¿ç”¨ transformers + bitsandbytes å®ç°å•å¡è¿è¡Œ
"""

import os
import sys
import json
import time
import base64
import tempfile
import torch
from flask import Flask, request, jsonify
from flask_cors import CORS

# è®¾ç½®ç¯å¢ƒ
os.environ["CUDA_VISIBLE_DEVICES"] = os.getenv("CUDA_DEVICE", "0")

app = Flask(__name__)
CORS(app)

# å…¨å±€æ¨¡å‹
model = None
tokenizer = None
processor = None

def load_model():
    """åŠ è½½ INT4 é‡åŒ–æ¨¡å‹"""
    global model, tokenizer
    
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    
    model_path = os.getenv("MODEL_PATH", "/model")
    
    print("=" * 60)
    print("Step-Audio-R1.1 INT4 é‡åŒ–æ¨ç†æœåŠ¡")
    print("=" * 60)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("=" * 60)
    
    # INT4 é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    print("\nåŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    print("åŠ è½½ INT4 é‡åŒ–æ¨¡å‹...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
    )
    
    print(f"\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print(f"æ˜¾å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

def process_audio(audio_path, prompt="Please transcribe this audio.", max_tokens=2048, temperature=0.7):
    """å¤„ç†éŸ³é¢‘"""
    import librosa
    
    # åŠ è½½éŸ³é¢‘
    audio, sr = librosa.load(audio_path, sr=16000)
    
    # æ„å»ºè¾“å…¥
    # Step-Audio ä½¿ç”¨ç‰¹æ®Šçš„éŸ³é¢‘æ ‡è®°
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio_url": audio_path},
                {"type": "text", "text": prompt}
            ]
        }
    ]
    
    # ä½¿ç”¨æ¨¡å‹çš„ chat æ–¹æ³•ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–ç›´æ¥ç”Ÿæˆ
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆ
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

@app.route('/health', methods=['GET'])
def health():
    return jsonify({"status": "healthy", "model": "Step-Audio-R1.1-INT4"})

@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({
        "object": "list",
        "data": [{
            "id": "Step-Audio-R1.1-INT4",
            "object": "model",
            "owned_by": "stepfun-ai",
            "quantization": "INT4-bitsandbytes"
        }]
    })

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """OpenAI å…¼å®¹çš„ chat completions API"""
    data = request.json
    messages = data.get('messages', [])
    max_tokens = data.get('max_tokens', 2048)
    temperature = data.get('temperature', 0.7)
    
    # æå–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(messages):
        if msg['role'] == 'user':
            content = msg['content']
            if isinstance(content, str):
                user_message = content
            elif isinstance(content, list):
                for item in content:
                    if item.get('type') == 'text':
                        user_message = item.get('text', '')
                        break
            break
    
    # ç”Ÿæˆå“åº”
    start_time = time.time()
    
    inputs = tokenizer(user_message, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature if temperature > 0 else 1.0,
            do_sample=temperature > 0,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    response_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
    elapsed = time.time() - start_time
    
    return jsonify({
        "id": f"chatcmpl-{int(time.time())}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": "Step-Audio-R1.1-INT4",
        "choices": [{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": response_text
            },
            "finish_reason": "stop"
        }],
        "usage": {
            "prompt_tokens": inputs['input_ids'].shape[1],
            "completion_tokens": outputs.shape[1] - inputs['input_ids'].shape[1],
            "total_tokens": outputs.shape[1]
        },
        "elapsed_time": elapsed
    })

@app.route('/process', methods=['POST'])
def process():
    """å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
    if 'audio' not in request.files:
        return jsonify({"error": "No audio file"}), 400
    
    audio_file = request.files['audio']
    prompt = request.form.get('prompt', 'Please transcribe this audio.')
    max_tokens = int(request.form.get('max_tokens', 2048))
    temperature = float(request.form.get('temperature', 0.7))
    
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
        audio_file.save(f.name)
        temp_path = f.name
    
    try:
        response = process_audio(temp_path, prompt, max_tokens, temperature)
        return jsonify({
            "status": "success",
            "response": response
        })
    finally:
        os.unlink(temp_path)

if __name__ == '__main__':
    load_model()
    
    port = int(os.getenv('PORT', 9998))
    print(f"\nğŸš€ æœåŠ¡å¯åŠ¨åœ¨ http://0.0.0.0:{port}")
    
    app.run(host='0.0.0.0', port=port, threaded=True)
